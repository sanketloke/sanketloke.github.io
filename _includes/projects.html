<section id="projects">
<div class="container">
  <h3>Projects</h3>
  <div class="panel panel-default">
    <div class="panel-body" >

    <div style="text-align:justify">
		<h5>
			<i class="fa fa-github"></i>&nbsp;&nbsp;<strong><a href="https://github.com/sanketloke/domain-adaptation">Domain Adaptation</a></strong>
			I am currently working on improving the generalization capabilities of deep learnt representations, for different domains. Domain adaptation in computer vision focuses on the task of reducing domain shift and generalizing across varied distributions. Adversarial learning methods have shown promise  for domain adaptation. Here, we investigate the use of CycleGAN, a promising new architecture on the task of domain adaptation in segmentation and classification.
		</h5>
	</div>

	<div style="text-align:justify"> 
		<h5>
			<i class="fa fa-github"></i>&nbsp;&nbsp;<strong><a href="https://github.com/sanketloke/qb-one"> QBOne: A Virtual Environment for Improving Quarterback Decisionmaking</a></strong>
			Decision of which receiver to pass the ball is the key element of success for a quarterback of the offensive team in American football. Good decision-making can be learnt through experience of multiple plays, but comes with the compounded risk of injury for the quarterback in each additional play. In this work, I developed Virtual Environment for football training focusing on the role of quarterback. Our goal with this work is to study the feasibility of using VR for training of decision-making in American football.
		</h5>
	</div>

	<div style="text-align:justify">
		<h5>
			<i class="fa fa-github"></i>&nbsp;&nbsp;<strong><a href="https://github.com/sanketloke/dlmultiagentsoccer">Deep Reinforcement Learning with Multi-agent Soccer</a></strong>
			We investigated an expanded version of Deep Q-Network where the reinforcement learning scheme is implemented to perform a
multi-agent task. In this approach, a new structure of neural network, the Deep Reinforcement Opponent Network (DRON) model is used in order to model the Q network for  the team and opponents’ policy simultaneously. [TODO: Switch to pytorch and integrate CommNet]
		</h5>

	</div>



	<div style="text-align:justify">

		<h5>
			<i class="fa fa-github"></i>&nbsp;&nbsp;<strong><a href="https://bit.ly/redisoptimization"> Structural Optimization For In-Memory Key-Value Stores</a></strong>
			In this project, we (Pratik Anand, Richa Sinha and myself) worked on 2 optimizations in Redis, specifically Sorted Set and String key-value Hash-Map,replacing them with more compact and memory efficient Adaptive Radix Tree and Google’s SparseHash. We performed evaluations on the modifications and validated the increased memory utilization seen due to the changes.
		</h5>
	</div>

	<div style="text-align:justify">
		<h5>
			<i class="fa fa-github"></i>&nbsp;&nbsp;<strong><a href="https://github.com/sanketloke/libconvex">Convex Optimization Library </a></strong>
			Built along with the coursework for CS5485: Numerical Optimization and Inverse Problems. 
			Contains implementation of the following algorithms from Nocedal.and Wright(1999) Numerical Optimization.
			<ol>
			<li> Line Search Methods </li>
			<li> Trust-Region Methods</li>
			<li> Conjugate Gradient </li>
			<li> Quasi-Newton </li>
			<li> Parameteric Least Squares</li>
			<li> Sequential Quadratic Programming for equality constraints </li>
			</ol>
		</h5>
	</div>

		</div>
    </div>
  </div>
</div>
</section>
